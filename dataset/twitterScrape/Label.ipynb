{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad64148b-151e-4b7c-8b9d-ec6b10b67af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pigeonXT as pixt\n",
    "\n",
    "csvs = os.listdir(\"newCsv/\")\n",
    "print(csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0005b51-e2fe-4ad3-8537-8befa62a9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa2d8a-b9a9-4410-9df1-554faac52374",
   "metadata": {},
   "outputs": [],
   "source": [
    "alreadyLabelled = []\n",
    "for file in os.listdir(\"output/\"):\n",
    "    if \".csv\" in file:\n",
    "        data = pd.read_csv(\"output/\"+file)\n",
    "        tweets = data[\"example\"].tolist()\n",
    "        alreadyLabelled += tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b61a14-1fc8-4573-866d-c7a2104bc28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetList = []\n",
    "for csv in csvs:\n",
    "    data = pd.read_csv(\"newCsv/\"+csv)\n",
    "    tweets = data[\"tweet\"].tolist()\n",
    "    tweetList += tweets\n",
    "\n",
    "tweetList = [x for x in tweetList if x not in alreadyLabelled]\n",
    "\n",
    "tweetList = random.sample(tweetList, len(tweetList))\n",
    "print(len(tweetList))\n",
    "chunk = tweetList[0:1000]#[tweetList[x:x+1000] for x in range(0, len(tweetList), 1000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62547298-4a7c-4b5f-9363-26f7139f9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pixt.annotate(chunk,options=['neither', 'political', 'provocative', 'unusable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db56dfa-bfa6-4a22-9f7a-a2fd22a0f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.to_csv(\"output/outputNewCategories00.csv\", columns=[\"example\", \"label\"], index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163c842-2eec-47f3-938a-aab75d72765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv(\"datasetFinal.csv\")\n",
    "print(f['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473226c-a595-4386-be09-03437bcdc373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "def preprocessData(dataset):\n",
    "    for i in range(len(dataset.example)):\n",
    "        removedHashtags = re.sub(r'(\\s)#\\w+', '', dataset.example[i])\n",
    "        removeAt = \" \".join([word for word in removedHashtags.split(\" \") if \"@\" not in word])\n",
    "        removeEmojis = emoji.replace_emoji(removeAt, \"\")#''.join(c for c in removeAt if c not in emoji.EMOJI)\n",
    "        final = re.sub(r'http\\S+', '', removeEmojis)\n",
    "        dataset.at[i, \"example\"] = final\n",
    "        \n",
    "    dataset = dataset[dataset['label'] != \"unusable\"]\n",
    "    return dataset\n",
    "\n",
    "ds = pd.read_csv(\"datasetFinal.csv\")\n",
    "processedDataset = preprocessData(ds)\n",
    "print(processedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaa65c-5744-4f65-94de-3b47643035ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PagMan",
   "language": "python",
   "name": "pagman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
